---
id: scaling-crawlers
title: Scaling our crawlers
description: To infinity and beyond! ...within limits
---

import ApiLink from '@site/src/components/ApiLink';

import CodeBlock from '@theme/CodeBlock';

import MaxRequestsPerMinuteSource from '!!raw-loader!./code/max_tasks_per_minute.py';
import MinMaxConcurrencySource from '!!raw-loader!./code/min_max_concurrency.py';


As we build our crawler, we might want to control how many tasks we do to the website at a time. Crawlee provides several options to fine tune how many parallel tasks should be made at any time, how many tasks should be done per minute, and how should scaling work based on the available system resources.

:::tip

All of these options are available on all crawlers Crawlee provides, but for this guide we'll be using the <ApiLink to="class/BeautifulSoupCrawler">`BeautifulSoupCrawler`</ApiLink>. We can see all the options for concurrency settings <ApiLink to="class/ConcurrencySettings">`here`</ApiLink>.
:::

## `max_tasks_per_minute`

This controls how many total tasks can be processed per minute. It counts the amount of tasks done every second, to ensure there is not a burst of tasks at the `max_concurrency` limit followed by a long period of waiting. By default, it is set to `Infinity` which means the crawler will keep going up to the `max_concurrency`. We would set this if we wanted our crawler to work at full throughput, but also not keep hitting the website we're crawling with non-stop requests.

<CodeBlock language="python">
    {MaxRequestsPerMinuteSource}
</CodeBlock>

## `min_concurrency` and `max_concurrency`

These control how many parallel tasks can be run at any time. By default, crawlers will start with one parallel task at a time and scale up over time to a maximum of 200 requests at a time.

:::caution Don't set `min_concurrency` too high!

Setting this option too high compared to the available system resources will make your crawler run extremely slow or might even crash.

It's recommended to leave it at the default value that is provided and letting the crawler scale up and down automatically based on available resources instead.

:::

## `desired_concurrency`

This option specifies the amount of tasks that should be running in parallel at the start of the crawler, assuming there are so many available. It defaults to the same value as `min_concurrency`.


<CodeBlock language="python">
    {MinMaxConcurrencySource}
</CodeBlock>

## `AutoscaledPool`

`AutoscaledPool` manages a pool of asynchronous resource-intensive tasks that are executed in parallel. The pool only starts new tasks if there is enough free CPU and memory available. If an exception is thrown in any of the tasks, it is propagated and the pool is stopped.

Internally each crawler has an `AutoscaledPool` which takes the above `ConcurrencySettings` as parameter to fine tune the operation. Some of the properties it has are-
- `desired_concurrency_ratio` - The minimum ratio of concurrency to reach before more scaling up is allowed (a number between `0` and `1`). By default, it is set to `0.9`.
- `scale_up_step_ratio` and `scale_down_step_ratio` - These values define the fractional amount of desired concurrency to be added or subtracted as the autoscaling pool scales up or down. Both of these values default to `0.05`.

